## Rock-Paper-Scissors Image Classification with MobileNetV2 Transfer Learning

#### This project demonstrates image classification for rock-paper-scissors gestures using MobileNetV2 transfer learning. The model achieves an accuracy of 88% on the test set, 91% on validation Set and 100% on training set.

Download the dataset:
The dataset used for training and evaluation is not included in this repository. Although you can download the dataset from Kaggle: https://www.kaggle.com/datasets/glushko/rock-paper-scissors-dataset.

Run the notebooks:
Open the Jupyter notebook environment and navigate to the desired notebook within the notebooks directory. Execute the code cells to train and evaluate the model. The 88% Accuracy model is already saved in a .keras file.

Note: You'll need to modify the file paths in the notebooks to point to the location where you extracted the dataset.

Further Exploration:

Explore the impact of hyperparameter tuning on model performance.
Implement data augmentation techniques to improve generalization.
Visualize the learned features by the model.
Feel free to reach out with any questions!
Dean
